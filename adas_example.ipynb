{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.optim import Adam\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adas import AdaLoRA, AdaLoRAMLP, AdaLoRAMLPWithBase, AdaLoRAWithBase\n",
    "\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# cifar traininig data\n",
    "def cifar_dataloader(batch_size, train=True, num_workers=8):\n",
    "    transform = []\n",
    "    if train:\n",
    "        transform.extend([\n",
    "            transforms.RandAugment(2, 14),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "    transform.extend([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform = transforms.Compose(transform)\n",
    "    # augs\n",
    "    train_loader = DataLoader(datasets.CIFAR10('data', \n",
    "                                train=train, \n",
    "                                download=True, \n",
    "                                transform=transform),\n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=train,\n",
    "                              num_workers=num_workers,\n",
    "                              )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x, **kwargs)) + x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., ada_lora_class=None, ada_lora_rank=8, ada_lora_sparse_heads=8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.adas = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            block = nn.ModuleList([\n",
    "                ResidualBlock(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                ResidualBlock(dim, FeedForward(dim, mlp_dim, dropout = dropout)),\n",
    "            ])\n",
    "            self.layers.append(block)\n",
    "            if ada_lora_class is not None:\n",
    "                self.adas.append(ada_lora_class(dim, ada_dim=dim, rank=ada_lora_rank, sparse_heads=ada_lora_sparse_heads))\n",
    "            else:\n",
    "                self.adas.append(Identity())\n",
    "    def forward(self, x):\n",
    "        for block, ada in zip(self.layers, self.adas):\n",
    "            for layer in block:\n",
    "                x = layer(x)\n",
    "            x = ada(x, ada_emb=x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size=32,\n",
    "                        patch_size=2,\n",
    "                        num_classes=10,\n",
    "                        dim=512,\n",
    "                        depth=6,\n",
    "                        heads=8,\n",
    "                        mlp_dim=2048,\n",
    "                        channels = 3,\n",
    "                        dim_head = 64, \n",
    "                        dropout = 0.1, \n",
    "                        emb_dropout = 0.1,\n",
    "                        ada_lora_class=None,\n",
    "                        ada_lora_rank=8,\n",
    "                        ada_lora_sparse_heads=8,\n",
    "                        ):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, \n",
    "                                        depth, \n",
    "                                        heads, \n",
    "                                        dim_head, \n",
    "                                        mlp_dim, \n",
    "                                        dropout,\n",
    "                                        ada_lora_class=ada_lora_class,\n",
    "                                        ada_lora_rank=ada_lora_rank,\n",
    "                                        ada_lora_sparse_heads=ada_lora_sparse_heads,\n",
    "                                        )\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ViT(\n",
      "  (to_patch_embedding): Sequential(\n",
      "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=4, p2=4)\n",
      "    (1): Linear(in_features=48, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (transformer): Transformer(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): Attention(\n",
      "            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (fn): FeedForward(\n",
      "            (net): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (adas): ModuleList(\n",
      "      (0-5): 6 x AdaLoRA(\n",
      "        (gen_weight): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=512, out_features=4096, bias=True)\n",
      "        )\n",
      "        (norm_cond): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_latent): Identity()\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.2076187133789062: 100%|██████████| 98/98 [00:38<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 2.3288506123484396 val loss: 2.1275402307510376 val acc: 0.184765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.1403934955596924: 100%|██████████| 98/98 [00:38<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 2.157498549441902 val loss: 2.0161254942417144 val acc: 0.23334099277853965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.0849263668060303: 100%|██████████| 98/98 [00:38<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 2.0754098892211914 val loss: 1.8564359068870544 val acc: 0.30101102963089943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9387891292572021: 100%|██████████| 98/98 [00:38<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 2.0202197846101253 val loss: 1.8041841685771942 val acc: 0.3329618573188782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.9917339086532593:  83%|████████▎ | 81/98 [00:31<00:06,  2.63it/s]"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from types import SimpleNamespace\n",
    "import datetime\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "args = SimpleNamespace(\n",
    "    batch_size=512,\n",
    "    epochs=100,\n",
    "    lr=1e-4,\n",
    "    rank=8,\n",
    "    ada_lora_class=AdaLoRA,\n",
    "    ada_lora_rank=4,\n",
    "    ada_lora_sparse_heads=None,\n",
    "    weight_decay=1e-3,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.99,\n",
    "    validate_every=1,\n",
    "    save_every=1,\n",
    "    use_fp16=False,\n",
    "    compile=False,\n",
    "    max_grad_norm=None,\n",
    "    lr_warmp_up_steps=100,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_loader = cifar_dataloader(args.batch_size, train=True)\n",
    "val_loader = cifar_dataloader(args.batch_size, train=False)\n",
    "model = ViT(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_classes=10,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=512,\n",
    "    channels = 3,\n",
    "    dim_head = 64, \n",
    "    dropout = 0.1, \n",
    "    emb_dropout = 0.1,\n",
    "    ada_lora_class=args.ada_lora_class,\n",
    "    ada_lora_rank=args.ada_lora_rank,\n",
    "    ada_lora_sparse_heads=args.ada_lora_sparse_heads,\n",
    "\n",
    ").to(device)\n",
    "print(model)\n",
    "model.train()\n",
    "optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, betas=(args.beta_1, args.beta_2))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "run_stats = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "scaler = torch.amp.GradScaler(enabled=args.use_fp16)\n",
    "\n",
    "if args.compile:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda x: min(1, x / args.lr_warmp_up_steps))\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    pbar = tqdm(train_loader)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(enabled=args.use_fp16, device_type='cuda'):\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        lr_scheduler.step()\n",
    "        # loss = do_step()\n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_description(f'loss: {loss.item()}')\n",
    "    run_stats['train_loss'].append(np.mean(train_losses))\n",
    "    if epoch % args.validate_every == 0:\n",
    "        model.eval()\n",
    "        results = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = model(x)\n",
    "                loss = criterion(y_hat, y)\n",
    "                y_pred = y_hat.argmax(dim=1)\n",
    "                results.append((y == y_pred).float().mean().item())\n",
    "                val_losses.append(loss.item())\n",
    "        run_stats['val_loss'].append(np.mean(val_losses))\n",
    "        run_stats['val_acc'].append(np.mean(results))\n",
    "        model.train()\n",
    "    if epoch % args.save_every == 0:\n",
    "        torch.save(model.state_dict(), f'logs/model_{epoch}.pth')\n",
    "    \n",
    "    print(f'Epoch {epoch} train loss: {run_stats[\"train_loss\"][-1]} val loss: {run_stats[\"val_loss\"][-1]} val acc: {run_stats[\"val_acc\"][-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
